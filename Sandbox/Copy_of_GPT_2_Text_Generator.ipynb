{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbloewencolon/Psychedelic-Trip-Generator/blob/main/Copy_of_GPT_2_Text_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e30aa2c0",
      "metadata": {
        "id": "e30aa2c0",
        "outputId": "2bfae0a2-ade6-433b-df65-b2142ce5ed01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "import pandas as pd\n",
        "import math\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from joblib import load\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.profiler import profile, record_function\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJnEf1BvAlUD",
        "outputId": "711be1a1-2d84-402e-be3b-25539e3f113a"
      },
      "id": "CJnEf1BvAlUD",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8022963-8db1-4818-ad7f-50f28d3dbda0",
      "metadata": {
        "id": "a8022963-8db1-4818-ad7f-50f28d3dbda0"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/modeled.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "edd28a94-294f-4f77-9bf2-7602a14daa38",
      "metadata": {
        "id": "edd28a94-294f-4f77-9bf2-7602a14daa38",
        "outputId": "1576ec3e-52ea-40f5-a329-2cd80088263a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 67516 entries, 0 to 67515\n",
            "Data columns (total 12 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   title              67516 non-null  object \n",
            " 1   drug               67516 non-null  object \n",
            " 2   dosage             67516 non-null  object \n",
            " 3   delivery           67516 non-null  object \n",
            " 4   weight             67516 non-null  float64\n",
            " 5   year               67516 non-null  int64  \n",
            " 6   gender             67516 non-null  object \n",
            " 7   report             67516 non-null  object \n",
            " 8   processed_report   67516 non-null  object \n",
            " 9   mixed              67516 non-null  int64  \n",
            " 10  drug_category      67516 non-null  object \n",
            " 11  report_embeddings  67516 non-null  object \n",
            "dtypes: float64(1), int64(2), object(9)\n",
            "memory usage: 6.2+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d9e6e857-392f-4cc9-bc1f-7b086b0388de",
      "metadata": {
        "id": "d9e6e857-392f-4cc9-bc1f-7b086b0388de",
        "outputId": "8a82110d-1c9f-4015-acfc-ef62a1af5166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer_path = '/content/drive/MyDrive/Colab Notebooks/Data/'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6397ca56-6f5b-42ee-a460-6e8ac4be4a0e",
      "metadata": {
        "id": "6397ca56-6f5b-42ee-a460-6e8ac4be4a0e"
      },
      "outputs": [],
      "source": [
        "unique_drugs = df['drug'].unique()\n",
        "special_tokens = [f'<{drug}>' for drug in unique_drugs]\n",
        "tokenizer.add_tokens(special_tokens)\n",
        "\n",
        "# Create a dictionary mapping each drug to its corresponding special token\n",
        "special_tokens_dict = {drug: f'<{drug}>' for drug in unique_drugs}\n",
        "\n",
        "# Apply the mapping to the 'report' column based on the 'drug' column\n",
        "df['report'] = df.apply(lambda row: special_tokens_dict[row['drug']] + ' ' + row['report'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "680df421-12cd-4ae5-b198-bccf7612c09c",
      "metadata": {
        "id": "680df421-12cd-4ae5-b198-bccf7612c09c"
      },
      "outputs": [],
      "source": [
        "model.resize_token_embeddings(len(tokenizer))\n",
        "vocab_size = len(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First Run\n",
        "#class GPT2Dataset(Dataset):\n",
        "#    def __init__(self, txt_list, tokenizer, max_length=1024):\n",
        "#        self.input_ids = []\n",
        "#        self.attn_masks = []\n",
        "#     for txt in txt_list:\n",
        "#        encodings_dict = tokenizer(txt, truncation=False, padding=False)\n",
        "#        tokens = encodings_dict['input_ids']\n",
        "#        self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "#        # Chunk the tokenized text into pieces of max_length\n",
        "#        for i in range(0, len(tokens), max_length):\n",
        "#            chunk = tokens[i: i + max_length]\n",
        "#            attention_mask = [1] * len(chunk)\n",
        "            # Padding\n",
        "#            while len(chunk) < max_length:\n",
        "#                chunk.append(0)\n",
        "#                attention_mask.append(0)\n",
        "#            self.input_ids.append(torch.tensor(chunk))\n",
        "#            self.attn_masks.append(torch.tensor(attention_mask))"
      ],
      "metadata": {
        "id": "XweYBJ9HDHW9"
      },
      "id": "XweYBJ9HDHW9",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4b452176-d453-4d14-b945-2c9602189057",
      "metadata": {
        "id": "4b452176-d453-4d14-b945-2c9602189057"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.load('/content/drive/MyDrive/Colab Notebooks/Data/input_ids.pt')\n",
        "attn_masks = torch.load('/content/drive/MyDrive/Colab Notebooks/Data/attn_masks.pt')\n",
        "\n",
        "class GPT2Dataset(Dataset):\n",
        "    def __init__(self, input_ids, attn_masks):\n",
        "        self.input_ids = input_ids\n",
        "        self.attn_masks = attn_masks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]\n",
        "\n",
        "dataset = GPT2Dataset(input_ids, attn_masks)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reports = df.report.values\n",
        "#dataset = GPT2Dataset(reports, tokenizer, max_length=1024)\n",
        "\n",
        "#torch.save(dataset.input_ids, 'D:/Cloud/Google Drive/Colab Notebooks/Data/input_ids.pt')\n",
        "#torch.save(dataset.attn_masks, 'D:/Cloud/Google Drive/Colab Notebooks/Data/attn_masks.pt')\n",
        "\n",
        "#tokenizer.save_pretrained('D:/Cloud/Google Drive/Colab Notebooks/Data/')"
      ],
      "metadata": {
        "id": "mzB_tgrxADe7"
      },
      "id": "mzB_tgrxADe7",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9a99233c-0d6a-4279-9f57-0c799c27a9c7",
      "metadata": {
        "id": "9a99233c-0d6a-4279-9f57-0c799c27a9c7"
      },
      "outputs": [],
      "source": [
        "# Splitting and loading as before\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "num_workers = 2\n",
        "batch_size = 16\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size, num_workers=num_workers)\n",
        "validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fcce530a-8bdc-467f-b2d5-2d64a5f5c4bb",
      "metadata": {
        "id": "fcce530a-8bdc-467f-b2d5-2d64a5f5c4bb"
      },
      "outputs": [],
      "source": [
        "# Function to generate text\n",
        "def generate_text(drug, length=500):\n",
        "    category = get_category_from_embedding(drug)  # Function to categorize the drug based on BigBird embeddings\n",
        "    input_str = f\"The experience of using {drug}, which belongs to the {category} category, is like...\"\n",
        "    inputs = tokenizer.encode(input_str, return_tensors='pt').to(device)\n",
        "    outputs = model.generate(inputs, max_length=length, num_return_sequences=1, do_sample=True)\n",
        "    return tokenizer.decode(outputs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b032e3f0-bc54-4e79-9bb1-97fadeeaf396",
      "metadata": {
        "id": "b032e3f0-bc54-4e79-9bb1-97fadeeaf396"
      },
      "outputs": [],
      "source": [
        "#tokenizer.save_pretrained('D:/Cloud/Google Drive/Colab Notebooks/Data/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95fee16d-781a-4ce8-b9cd-6ad76df674da",
      "metadata": {
        "id": "95fee16d-781a-4ce8-b9cd-6ad76df674da"
      },
      "outputs": [],
      "source": [
        "from torch.profiler import profile, record_function\n",
        "\n",
        "# Define number of epochs\n",
        "epochs = 3\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * epochs)\n",
        "\n",
        "accumulation_steps = 4\n",
        "\n",
        "# Create a profiler context\n",
        "with profile(activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Starting epoch\", epoch+1)\n",
        "        model.train()\n",
        "        print(\"Model set to training mode\")\n",
        "        optimizer.zero_grad()\n",
        "        print(\"Gradients reset\")\n",
        "\n",
        "            for batch_idx, batch in enumerate(train_dataloader):\n",
        "                print(f\"Starting batch {batch_idx}\")\n",
        "            # Optional: use record_function to label sections of your code\n",
        "            with record_function(\"Load data\"):\n",
        "                print(\"Processing batch\", batch_idx) # Debug print\n",
        "                inputs, masks = batch\n",
        "                print(\"Data extracted from batch\") # Debug print\n",
        "                inputs, masks = inputs.to(device), masks.to(device)\n",
        "                print(\"Data moved to GPU\") # Debug print\n",
        "\n",
        "            with record_function(\"Forward pass\"):\n",
        "                outputs = model(inputs, attention_mask=masks, labels=inputs)\n",
        "                loss = outputs.loss\n",
        "\n",
        "            with record_function(\"Backward pass\"):\n",
        "                loss.backward()\n",
        "\n",
        "                if (batch_idx + 1) % accumulation_steps == 0:  # Only update every accumulation_steps\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                processed_percentage = (batch_idx + 1) / len(train_dataloader) * 100\n",
        "                print(f'\\rProcessed: {processed_percentage:.2f}% of data in epoch {epoch + 1}', end='')\n",
        "\n",
        "        print(f'\\nEpoch {epoch + 1}: Training Loss: {loss.item()}')\n",
        "        sample_text = generate_text(\"Sample prompt\", length=50)\n",
        "        print(f\"Generated Text: {sample_text}\")\n",
        "\n",
        "    print(f\"Finished Epoch {epoch+1}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'D:/Cloud/Google Drive/Colab Notebooks/Data/trip_reports_model.pth')\n",
        "\n",
        "# Print profiler results\n",
        "prof.export_chrome_trace(\"trace.json\")\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e782b980-8e6e-4a9c-bf9f-d9d92a84205d",
      "metadata": {
        "id": "e782b980-8e6e-4a9c-bf9f-d9d92a84205d"
      },
      "outputs": [],
      "source": [
        "# Validation loop\n",
        "model.eval()\n",
        "total_eval_loss = 0\n",
        "eval_steps = 0\n",
        "with torch.no_grad():\n",
        "    for batch in validation_dataloader:\n",
        "        inputs, masks = batch\n",
        "        inputs, masks = inputs.to(device), masks.to(device)\n",
        "        outputs = model(inputs, attention_mask=masks, labels=inputs)\n",
        "        loss = outputs.loss\n",
        "        total_eval_loss += loss.item()\n",
        "        eval_steps += 1\n",
        "\n",
        "avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "perplexity = math.exp(avg_val_loss)  # Compute perplexity from the average loss\n",
        "\n",
        "print(f'Validation Loss: {avg_val_loss}')\n",
        "print(f'Validation Perplexity: {perplexity}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63c2493-a23d-40b6-8a82-5d409e71a4c7",
      "metadata": {
        "id": "a63c2493-a23d-40b6-8a82-5d409e71a4c7"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d43fd8-f960-4d4a-ac7a-eceaa8c7751a",
      "metadata": {
        "id": "b0d43fd8-f960-4d4a-ac7a-eceaa8c7751a"
      },
      "outputs": [],
      "source": [
        "def generate_report(drug, desired_length_min=300, desired_length_max=500):\n",
        "    # Step 1: Preprocess\n",
        "    output_file = 'D:/Cloud/Google Drive/Colab Notebooks/Data/bigbird_embeddings.joblib'\n",
        "    bigbird_embeddings = load(output_file)\n",
        "    embedding = bigbird_embeddings[drug]\n",
        "\n",
        "    # Step 2: Generate text with GPT-2\n",
        "    text = generate_text(drug)\n",
        "\n",
        "    # Step 3: Evaluate with RFC (optional)\n",
        "    # Load the trained Random Forest model\n",
        "    with open(\"D:/Cloud/Google Drive/Colab Notebooks/Data/xgb_model.pkl\", \"rb\") as f:\n",
        "    rfc_model = pickle.load(f)\n",
        "    vectorized_text = tfidf_vectorizer.transform([text])\n",
        "    category_prediction = rfc_model.predict(vectorized_text)\n",
        "    # Validate or modify text based on category prediction if needed\n",
        "\n",
        "    # Step 4: Post-process text\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Concatenate sentences until you reach the desired minimum length\n",
        "    processed_text = \"\"\n",
        "    for sentence in sentences:\n",
        "        processed_text += sentence + \" \"\n",
        "        words = processed_text.split()\n",
        "        if len(words) >= desired_length_min:\n",
        "            break\n",
        "\n",
        "    # If the text exceeds the desired maximum length, truncate it\n",
        "     words = text.split()\n",
        "    if len(words) > 4000:\n",
        "        text = \" \".join(words[:4000])\n",
        "    if len(words) > desired_length_max:\n",
        "        processed_text = \" \".join(words[:desired_length_max])\n",
        "\n",
        "    return processed_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81d0dc58-69b2-4562-aec8-eb78af9af36e",
      "metadata": {
        "id": "81d0dc58-69b2-4562-aec8-eb78af9af36e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
