{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbloewencolon/Psychedelic-Trip-Generator/blob/main/GPT_2_Text_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e30aa2c0",
      "metadata": {
        "id": "e30aa2c0",
        "outputId": "2baf7638-4957-4ece-8dfe-7a88648b5e2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA GeForce GTX 1070\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "import pandas as pd\n",
        "import math\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from joblib import load\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8022963-8db1-4818-ad7f-50f28d3dbda0",
      "metadata": {
        "id": "a8022963-8db1-4818-ad7f-50f28d3dbda0"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv('D:/Cloud/Google Drive/Colab Notebooks/Data/modeled.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd28a94-294f-4f77-9bf2-7602a14daa38",
      "metadata": {
        "id": "edd28a94-294f-4f77-9bf2-7602a14daa38",
        "outputId": "1697612a-97f9-47ac-be8b-9dffb5a4baeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 67516 entries, 0 to 67515\n",
            "Data columns (total 12 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   title              67516 non-null  object \n",
            " 1   drug               67516 non-null  object \n",
            " 2   dosage             67516 non-null  object \n",
            " 3   delivery           67516 non-null  object \n",
            " 4   weight             67516 non-null  float64\n",
            " 5   year               67516 non-null  int64  \n",
            " 6   gender             67516 non-null  object \n",
            " 7   report             67516 non-null  object \n",
            " 8   processed_report   67516 non-null  object \n",
            " 9   mixed              67516 non-null  int64  \n",
            " 10  drug_category      67516 non-null  object \n",
            " 11  report_embeddings  67516 non-null  object \n",
            "dtypes: float64(1), int64(2), object(9)\n",
            "memory usage: 6.2+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9e6e857-392f-4cc9-bc1f-7b086b0388de",
      "metadata": {
        "id": "d9e6e857-392f-4cc9-bc1f-7b086b0388de",
        "outputId": "3b81f10b-6226-467e-b1f4-62ada0940468"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6397ca56-6f5b-42ee-a460-6e8ac4be4a0e",
      "metadata": {
        "id": "6397ca56-6f5b-42ee-a460-6e8ac4be4a0e"
      },
      "outputs": [],
      "source": [
        "unique_drugs = df['drug'].unique()\n",
        "special_tokens = [f'<{drug}>' for drug in unique_drugs]\n",
        "tokenizer.add_tokens(special_tokens)\n",
        "\n",
        "# Create a dictionary mapping each drug to its corresponding special token\n",
        "special_tokens_dict = {drug: f'<{drug}>' for drug in unique_drugs}\n",
        "\n",
        "# Apply the mapping to the 'report' column based on the 'drug' column\n",
        "df['report'] = df.apply(lambda row: special_tokens_dict[row['drug']] + ' ' + row['report'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "680df421-12cd-4ae5-b198-bccf7612c09c",
      "metadata": {
        "id": "680df421-12cd-4ae5-b198-bccf7612c09c"
      },
      "outputs": [],
      "source": [
        "model.resize_token_embeddings(len(tokenizer))\n",
        "vocab_size = len(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b452176-d453-4d14-b945-2c9602189057",
      "metadata": {
        "id": "4b452176-d453-4d14-b945-2c9602189057"
      },
      "outputs": [],
      "source": [
        "class GPT2Dataset(Dataset):\n",
        "    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        for txt in txt_list:\n",
        "            encodings_dict = tokenizer(txt, truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcce530a-8bdc-467f-b2d5-2d64a5f5c4bb",
      "metadata": {
        "id": "fcce530a-8bdc-467f-b2d5-2d64a5f5c4bb"
      },
      "outputs": [],
      "source": [
        "# Function to generate text\n",
        "def generate_text(drug, length=500):\n",
        "    category = get_category_from_embedding(drug)  # Function to categorize the drug based on BigBird embeddings\n",
        "    input_str = f\"The experience of using {drug}, which belongs to the {category} category, is like...\"\n",
        "    inputs = tokenizer.encode(input_str, return_tensors='pt').to(device)\n",
        "    outputs = model.generate(inputs, max_length=length, num_return_sequences=1, do_sample=True)\n",
        "    return tokenizer.decode(outputs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a99233c-0d6a-4279-9f57-0c799c27a9c7",
      "metadata": {
        "id": "9a99233c-0d6a-4279-9f57-0c799c27a9c7"
      },
      "outputs": [],
      "source": [
        "reports = df.report.values\n",
        "dataset = GPT2Dataset(reports, tokenizer, max_length=768)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "num_workers = 4\n",
        "batch_size = 16\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size, num_workers=num_workers)\n",
        "validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b032e3f0-bc54-4e79-9bb1-97fadeeaf396",
      "metadata": {
        "id": "b032e3f0-bc54-4e79-9bb1-97fadeeaf396",
        "outputId": "a1bf8225-c46b-4bdb-9a87-b1ff14af50a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('D:/Cloud/Google Drive/Colab Notebooks/Data/tokenizer_config.json',\n",
              " 'D:/Cloud/Google Drive/Colab Notebooks/Data/special_tokens_map.json',\n",
              " 'D:/Cloud/Google Drive/Colab Notebooks/Data/vocab.json',\n",
              " 'D:/Cloud/Google Drive/Colab Notebooks/Data/merges.txt',\n",
              " 'D:/Cloud/Google Drive/Colab Notebooks/Data/added_tokens.json')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained('D:/Cloud/Google Drive/Colab Notebooks/Data/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95fee16d-781a-4ce8-b9cd-6ad76df674da",
      "metadata": {
        "id": "95fee16d-781a-4ce8-b9cd-6ad76df674da",
        "outputId": "78cc3e54-8307-469f-8cd0-a8fa79f0b795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: 0.01% of data in epoch 1\n",
            "Processed: 9.99% of data in epoch 1\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Clip gradients\u001b[39;00m\n\u001b[0;32m     27\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define number of epochs\n",
        "epochs = 3\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * epochs)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_batches = len(train_dataloader)\n",
        "    print_every = total_batches // 10  # Print every 10%\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        inputs, masks = batch\n",
        "        inputs, masks = inputs.to(device), masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, attention_mask=masks, labels=inputs)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Print how much of the data has been processed every 10%\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'\\rProcessed: {processed_percentage:.2f}% of data in epoch {epoch + 1}', end='')\n",
        "\n",
        "    # Print training loss and generate sample text\n",
        "    print(f'Epoch {epoch + 1}: Training Loss: {loss.item()}')\n",
        "    sample_text = generate_text(\"Sample prompt\", length=50)\n",
        "    print(f\"Generated Text: {sample_text}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'D:/Cloud/Google Drive/Colab Notebooks/Data/trip_reports_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e782b980-8e6e-4a9c-bf9f-d9d92a84205d",
      "metadata": {
        "id": "e782b980-8e6e-4a9c-bf9f-d9d92a84205d"
      },
      "outputs": [],
      "source": [
        "# Validation loop\n",
        "model.eval()\n",
        "total_eval_loss = 0\n",
        "eval_steps = 0\n",
        "with torch.no_grad():\n",
        "    for batch in validation_dataloader:\n",
        "        inputs, masks = batch\n",
        "        inputs, masks = inputs.to(device), masks.to(device)\n",
        "        outputs = model(inputs, attention_mask=masks, labels=inputs)\n",
        "        loss = outputs.loss\n",
        "        total_eval_loss += loss.item()\n",
        "        eval_steps += 1\n",
        "\n",
        "avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "perplexity = math.exp(avg_val_loss)  # Compute perplexity from the average loss\n",
        "\n",
        "print(f'Validation Loss: {avg_val_loss}')\n",
        "print(f'Validation Perplexity: {perplexity}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63c2493-a23d-40b6-8a82-5d409e71a4c7",
      "metadata": {
        "id": "a63c2493-a23d-40b6-8a82-5d409e71a4c7"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d43fd8-f960-4d4a-ac7a-eceaa8c7751a",
      "metadata": {
        "id": "b0d43fd8-f960-4d4a-ac7a-eceaa8c7751a"
      },
      "outputs": [],
      "source": [
        "def generate_report(drug, desired_length_min=300, desired_length_max=500):\n",
        "    # Step 1: Preprocess\n",
        "    output_file = 'D:/Cloud/Google Drive/Colab Notebooks/Data/bigbird_embeddings.joblib'\n",
        "    bigbird_embeddings = load(output_file)\n",
        "    embedding = bigbird_embeddings[drug]\n",
        "\n",
        "    # Step 2: Generate text with GPT-2\n",
        "    text = generate_text(drug)\n",
        "\n",
        "    # Step 3: Evaluate with RFC (optional)\n",
        "    # Load the trained Random Forest model\n",
        "    with open(\"D:/Cloud/Google Drive/Colab Notebooks/Data/xgb_model.pkl\", \"rb\") as f:\n",
        "    rfc_model = pickle.load(f)\n",
        "    vectorized_text = tfidf_vectorizer.transform([text])\n",
        "    category_prediction = rfc_model.predict(vectorized_text)\n",
        "    # Validate or modify text based on category prediction if needed\n",
        "\n",
        "    # Step 4: Post-process text\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Concatenate sentences until you reach the desired minimum length\n",
        "    processed_text = \"\"\n",
        "    for sentence in sentences:\n",
        "        processed_text += sentence + \" \"\n",
        "        words = processed_text.split()\n",
        "        if len(words) >= desired_length_min:\n",
        "            break\n",
        "\n",
        "    # If the text exceeds the desired maximum length, truncate it\n",
        "     words = text.split()\n",
        "    if len(words) > 4000:\n",
        "        text = \" \".join(words[:4000])\n",
        "    if len(words) > desired_length_max:\n",
        "        processed_text = \" \".join(words[:desired_length_max])\n",
        "\n",
        "    return processed_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81d0dc58-69b2-4562-aec8-eb78af9af36e",
      "metadata": {
        "id": "81d0dc58-69b2-4562-aec8-eb78af9af36e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "pytorch_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}