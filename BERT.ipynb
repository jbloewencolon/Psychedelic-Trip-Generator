{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true,
      "mount_file_id": "1G_HBjSnrJAx4Sd--fgYycIWWtnqQNHXD",
      "authorship_tag": "ABX9TyOsXs4tsctBaBX7fDESR+UP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbloewencolon/Psychedelic-Trip-Generator/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3L8ThvWC-cvL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import torch\n",
        "import xgboost as xgb\n",
        "import os\n",
        "from joblib import dump, load\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch import device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to load pre-trained model and tokenizer\n",
        "def load_pretrained_model_and_tokenizer(model_file, tokenizer_file):\n",
        "    # Check if the model and tokenizer are already saved\n",
        "    if os.path.exists(model_file) and os.path.exists(tokenizer_file):\n",
        "        model = load(model_file)\n",
        "        tokenizer = load(tokenizer_file)\n",
        "    else:\n",
        "        # Load pre-trained model tokenizer\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Load pre-trained model (weights)\n",
        "        model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
        "        model = model.to(device)  # Move model to GPU if available\n",
        "        model.eval()  # Put the model in \"evaluation\" mode\n",
        "\n",
        "        # Save the model and tokenizer files\n",
        "        dump(model, model_file)\n",
        "        dump(tokenizer, tokenizer_file)\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "_061xOXsffZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('D:/Cloud/Google Drive/Colab Notebooks/Data/processed.csv')"
      ],
      "metadata": {
        "id": "H590US3U_Zen"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory where you want to save the file\n",
        "output_dir = 'D:/Cloud/Google Drive/Colab Notebooks/Data'\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it does not exist\n",
        "\n",
        "model_file = os.path.join(output_dir, \"bert_model.joblib\")\n",
        "tokenizer_file = os.path.join(output_dir, \"bert_tokenizer.joblib\")\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model, tokenizer = load_pretrained_model_and_tokenizer(model_file, tokenizer_file)"
      ],
      "metadata": {
        "id": "Y7jWJcbbfqCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "mzo_TlQNG-x_",
        "outputId": "aa0f2c30-ff81-488c-fcfa-d63797cef8d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 76450 entries, 0 to 76449\n",
            "Data columns (total 11 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   title             76448 non-null  object \n",
            " 1   drug              76447 non-null  object \n",
            " 2   dosage            73699 non-null  object \n",
            " 3   delivery          74213 non-null  object \n",
            " 4   weight            76449 non-null  float64\n",
            " 5   year              76449 non-null  float64\n",
            " 6   gender            76446 non-null  object \n",
            " 7   report            76439 non-null  object \n",
            " 8   processed_report  76438 non-null  object \n",
            " 9   mixed             76448 non-null  float64\n",
            " 10  drug_category     76448 non-null  object \n",
            "dtypes: float64(3), object(8)\n",
            "memory usage: 6.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values with 'unknown' in 'drug_category' column\n",
        "df[['drug', 'dosage', 'delivery']] = df[['drug', 'dosage', 'delivery']].fillna('unknown')\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbhusvMOIXe1",
        "outputId": "60962344-6d58-488e-c018-a08af6da72db"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 76438 entries, 0 to 76449\n",
            "Data columns (total 11 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   title             76438 non-null  object \n",
            " 1   drug              76438 non-null  object \n",
            " 2   dosage            76438 non-null  object \n",
            " 3   delivery          76438 non-null  object \n",
            " 4   weight            76438 non-null  float64\n",
            " 5   year              76438 non-null  float64\n",
            " 6   gender            76438 non-null  object \n",
            " 7   report            76438 non-null  object \n",
            " 8   processed_report  76438 non-null  object \n",
            " 9   mixed             76438 non-null  float64\n",
            " 10  drug_category     76438 non-null  object \n",
            "dtypes: float64(3), object(8)\n",
            "memory usage: 7.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "uXoi-tsyHAnO",
        "outputId": "1c630cdf-b7da-4d52-b432-55f46608ad3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   title      drug       dosage      delivery  \\\n",
              "0                             Ode to Joy      mdma  1.5 tablets          oral   \n",
              "1  Make Sure the Music's Not Too Complex  cannabis      unknown        smoked   \n",
              "2                            After Hours      mdma       160 mg          oral   \n",
              "3                            After Hours      mdma       100 mg          oral   \n",
              "4                            After Hours      mdma        50 mg   insufflated   \n",
              "\n",
              "   weight    year         gender  \\\n",
              "0   185.0  2000.0           male   \n",
              "1   152.0  1999.0  not specified   \n",
              "2   150.0  2001.0           male   \n",
              "3   150.0  2001.0           male   \n",
              "4   150.0  2001.0           male   \n",
              "\n",
              "                                              report  \\\n",
              "0  My friend had some experience with X and had t...   \n",
              "1  This was the first experience that either my f...   \n",
              "2  Preparation: I have heard some conflicting opi...   \n",
              "3  Preparation: I have heard some conflicting opi...   \n",
              "4  Preparation: I have heard some conflicting opi...   \n",
              "\n",
              "                                    processed_report  mixed drug_category  \n",
              "0  friend experi x told one day said come across ...    0.0          mdma  \n",
              "1  first experi either friend salvia housem check...    0.0      cannabis  \n",
              "2  prepar heard conflict opinion 5htp ie load day...    1.0          mdma  \n",
              "3  prepar heard conflict opinion 5htp ie load day...    1.0          mdma  \n",
              "4  prepar heard conflict opinion 5htp ie load day...    1.0          mdma  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>drug</th>\n",
              "      <th>dosage</th>\n",
              "      <th>delivery</th>\n",
              "      <th>weight</th>\n",
              "      <th>year</th>\n",
              "      <th>gender</th>\n",
              "      <th>report</th>\n",
              "      <th>processed_report</th>\n",
              "      <th>mixed</th>\n",
              "      <th>drug_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ode to Joy</td>\n",
              "      <td>mdma</td>\n",
              "      <td>1.5 tablets</td>\n",
              "      <td>oral</td>\n",
              "      <td>185.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>male</td>\n",
              "      <td>My friend had some experience with X and had t...</td>\n",
              "      <td>friend experi x told one day said come across ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>mdma</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Make Sure the Music's Not Too Complex</td>\n",
              "      <td>cannabis</td>\n",
              "      <td>unknown</td>\n",
              "      <td>smoked</td>\n",
              "      <td>152.0</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>not specified</td>\n",
              "      <td>This was the first experience that either my f...</td>\n",
              "      <td>first experi either friend salvia housem check...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>cannabis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>After Hours</td>\n",
              "      <td>mdma</td>\n",
              "      <td>160 mg</td>\n",
              "      <td>oral</td>\n",
              "      <td>150.0</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>male</td>\n",
              "      <td>Preparation: I have heard some conflicting opi...</td>\n",
              "      <td>prepar heard conflict opinion 5htp ie load day...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>mdma</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>After Hours</td>\n",
              "      <td>mdma</td>\n",
              "      <td>100 mg</td>\n",
              "      <td>oral</td>\n",
              "      <td>150.0</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>male</td>\n",
              "      <td>Preparation: I have heard some conflicting opi...</td>\n",
              "      <td>prepar heard conflict opinion 5htp ie load day...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>mdma</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>After Hours</td>\n",
              "      <td>mdma</td>\n",
              "      <td>50 mg</td>\n",
              "      <td>insufflated</td>\n",
              "      <td>150.0</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>male</td>\n",
              "      <td>Preparation: I have heard some conflicting opi...</td>\n",
              "      <td>prepar heard conflict opinion 5htp ie load day...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>mdma</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['drug_category'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmHK62eNuVal",
        "outputId": "e53b98bd-a7ca-4891-d6c0-9910e9495cdc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pharmaceutical           10125\n",
              "cannabis                  9536\n",
              "stimulant                 5780\n",
              "mushrooms                 4055\n",
              "botanical                 3814\n",
              "opioid                    3795\n",
              "mdma                      3560\n",
              "alcohol                   3368\n",
              "hallucinogen              3128\n",
              "lsd                       3064\n",
              "salvia                    2831\n",
              "2c                        2812\n",
              "other                     2746\n",
              "dissociative              2624\n",
              "entheogen                 2546\n",
              "DMT                       2362\n",
              "entactogen                1754\n",
              "nootropic                 1653\n",
              "5-meo                     1290\n",
              "ketamine                  1233\n",
              "unknown                    923\n",
              "mescaline                  726\n",
              "depressant                 686\n",
              "synthetic cannabinoid      451\n",
              "anxiolytic                 448\n",
              "oneirogen                  307\n",
              "ayahuasca                  229\n",
              "phencyclidine              216\n",
              "antidepressant             167\n",
              "3-MeO                      131\n",
              "ibogaine                    78\n",
              "Name: drug_category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mapping from old categories to new ones\n",
        "category_mapping = {\n",
        "    'pharmaceutical': 'Pharmaceutical',\n",
        "    'cannabis': 'Cannabinoid',\n",
        "    'stimulant': 'Stimulant',\n",
        "    'mushrooms': 'Psychedelic',\n",
        "    'botanical': 'Other',\n",
        "    'opioid': 'Opioid',\n",
        "    'mdma': 'Entactogen/Empathogen',\n",
        "    'alcohol': 'Depressant',\n",
        "    'hallucinogen': 'Psychedelic',\n",
        "    'lsd': 'Psychedelic',\n",
        "    'salvia': 'Psychedelic',\n",
        "    '2c': 'Psychedelic',\n",
        "    'other': 'Other',\n",
        "    'dissociative': 'Dissociative',\n",
        "    'entheogen': 'Entheogen',\n",
        "    'DMT': 'Psychedelic',\n",
        "    'entactogen': 'Entactogen/Empathogen',\n",
        "    'nootropic': 'Other',\n",
        "    '5-meo': 'Psychedelic',\n",
        "    'ketamine': 'Dissociative',\n",
        "    'unknown': 'Other',\n",
        "    'mescaline': 'Psychedelic',\n",
        "    'depressant': 'Depressant',\n",
        "    'synthetic cannabinoid': 'Cannabinoid',\n",
        "    'anxiolytic': 'Pharmaceutical',\n",
        "    'oneirogen': 'Other',\n",
        "    'ayahuasca': 'Psychedelic',\n",
        "    'phencyclidine': 'Dissociative',\n",
        "    'antidepressant': 'Pharmaceutical',\n",
        "    '3-MeO': 'Other',\n",
        "    'ibogaine': 'Entheogen'\n",
        "}\n",
        "\n",
        "# Apply the mapping to the 'drug_category' column and create a new column 'grouped_drug_category'\n",
        "df['drug_category'] = df['drug_category'].map(category_mapping)\n",
        "\n",
        "df['drug_category'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vX5jkdztN1V",
        "outputId": "5b431763-f5fa-4484-f225-2c16c214b3a6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Psychedelic              20497\n",
              "Pharmaceutical           10740\n",
              "Cannabinoid               9987\n",
              "Other                     9574\n",
              "Stimulant                 5780\n",
              "Entactogen/Empathogen     5314\n",
              "Dissociative              4073\n",
              "Depressant                4054\n",
              "Opioid                    3795\n",
              "Entheogen                 2624\n",
              "Name: drug_category, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IHoP7dFy9KVx",
        "outputId": "25013c1b-c967-40a3-e3d5-a5f3371dda5a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   title      drug       dosage      delivery  \\\n",
              "0                             Ode to Joy      mdma  1.5 tablets          oral   \n",
              "1  Make Sure the Music's Not Too Complex  cannabis      unknown        smoked   \n",
              "2                            After Hours      mdma       160 mg          oral   \n",
              "3                            After Hours      mdma       100 mg          oral   \n",
              "4                            After Hours      mdma        50 mg   insufflated   \n",
              "\n",
              "   weight    year         gender  \\\n",
              "0   185.0  2000.0           male   \n",
              "1   152.0  1999.0  not specified   \n",
              "2   150.0  2001.0           male   \n",
              "3   150.0  2001.0           male   \n",
              "4   150.0  2001.0           male   \n",
              "\n",
              "                                              report  \\\n",
              "0  My friend had some experience with X and had t...   \n",
              "1  This was the first experience that either my f...   \n",
              "2  Preparation: I have heard some conflicting opi...   \n",
              "3  Preparation: I have heard some conflicting opi...   \n",
              "4  Preparation: I have heard some conflicting opi...   \n",
              "\n",
              "                                    processed_report  mixed  \\\n",
              "0  friend experi x told one day said come across ...    0.0   \n",
              "1  first experi either friend salvia housem check...    0.0   \n",
              "2  prepar heard conflict opinion 5htp ie load day...    1.0   \n",
              "3  prepar heard conflict opinion 5htp ie load day...    1.0   \n",
              "4  prepar heard conflict opinion 5htp ie load day...    1.0   \n",
              "\n",
              "           drug_category  \n",
              "0  Entactogen/Empathogen  \n",
              "1            Cannabinoid  \n",
              "2  Entactogen/Empathogen  \n",
              "3  Entactogen/Empathogen  \n",
              "4  Entactogen/Empathogen  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>drug</th>\n",
              "      <th>dosage</th>\n",
              "      <th>delivery</th>\n",
              "      <th>weight</th>\n",
              "      <th>year</th>\n",
              "      <th>gender</th>\n",
              "      <th>report</th>\n",
              "      <th>processed_report</th>\n",
              "      <th>mixed</th>\n",
              "      <th>drug_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ode to Joy</td>\n",
              "      <td>mdma</td>\n",
              "      <td>1.5 tablets</td>\n",
              "      <td>oral</td>\n",
              "      <td>185.0</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>male</td>\n",
              "      <td>My friend had some experience with X and had t...</td>\n",
              "      <td>friend experi x told one day said come across ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Entactogen/Empathogen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Make Sure the Music's Not Too Complex</td>\n",
              "      <td>cannabis</td>\n",
              "      <td>unknown</td>\n",
              "      <td>smoked</td>\n",
              "      <td>152.0</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>not specified</td>\n",
              "      <td>This was the first experience that either my f...</td>\n",
              "      <td>first experi either friend salvia housem check...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Cannabinoid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>After Hours</td>\n",
              "      <td>mdma</td>\n",
              "      <td>160 mg</td>\n",
              "      <td>oral</td>\n",
              "      <td>150.0</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>male</td>\n",
              "      <td>Preparation: I have heard some conflicting opi...</td>\n",
              "      <td>prepar heard conflict opinion 5htp ie load day...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Entactogen/Empathogen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>After Hours</td>\n",
              "      <td>mdma</td>\n",
              "      <td>100 mg</td>\n",
              "      <td>oral</td>\n",
              "      <td>150.0</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>male</td>\n",
              "      <td>Preparation: I have heard some conflicting opi...</td>\n",
              "      <td>prepar heard conflict opinion 5htp ie load day...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Entactogen/Empathogen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>After Hours</td>\n",
              "      <td>mdma</td>\n",
              "      <td>50 mg</td>\n",
              "      <td>insufflated</td>\n",
              "      <td>150.0</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>male</td>\n",
              "      <td>Preparation: I have heard some conflicting opi...</td>\n",
              "      <td>prepar heard conflict opinion 5htp ie load day...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Entactogen/Empathogen</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device to GPU (if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "pR-28yaQvouJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(text, tokenizer, model):\n",
        "    # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for BERT\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "    # Check tokenized text length\n",
        "    if len(tokenized_text) > 512:\n",
        "        tokenized_text = tokenized_text[:512]  # Truncate if too long\n",
        "\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "    segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
        "    segments_tensors = torch.tensor([segments_ids]).to(device)\n",
        "\n",
        "    # Predict hidden states features for each layer\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, segments_tensors)\n",
        "        hidden_states = outputs.hidden_states\n",
        "\n",
        "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "    token_embeddings = token_embeddings.permute(1,0,2)\n",
        "\n",
        "    # Sum the vectors from the last four layers.\n",
        "    sum_vec = torch.sum(token_embeddings[-4:], dim=0)\n",
        "\n",
        "    return sum_vec"
      ],
      "metadata": {
        "id": "zsXtzaXLeC20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the BERT embeddings for each report\n",
        "df['processed_report'] = df['processed_report'].astype(str)\n",
        "processed_reports = df['processed_report'].tolist()\n",
        "bert_embeddings = [get_bert_embeddings(report, tokenizer, model) for report in processed_reports]"
      ],
      "metadata": {
        "id": "zghgQQSUVIce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tensors to numpy arrays for pickling\n",
        "bert_embeddings = [embedding.cpu().numpy() for embedding in bert_embeddings]\n",
        "\n",
        "# Assign the embeddings back to the dataframe\n",
        "df['report_embeddings'] = bert_embeddings"
      ],
      "metadata": {
        "id": "ptNGev_N690a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the full path of the file\n",
        "output_file = os.path.join(output_dir, 'bert_embeddings.joblib')\n",
        "\n",
        "# Save the embeddings to a file\n",
        "with open(output_file, 'wb') as f:\n",
        "    dump(bert_embeddings, f)"
      ],
      "metadata": {
        "id": "jEtbkI877L-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert list of tensors to numpy array\n",
        "df['report_embeddings'] = df['report_embeddings'].apply(lambda x: np.mean([t.numpy() for t in x], axis=0))\n",
        "\n",
        "# Split data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['report_embeddings'].to_list(), df['drug_category'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "UQZVvW8Jx1Pv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "outputId": "89b0088a-d1c4-4cee-ad41-58dc77fa9cbb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32m~\\miniconda3\\envs\\learn-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\learn-env\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\learn-env\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'report_embeddings'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert list of tensors to numpy array\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreport_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreport_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39mmean([t\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Split data into train and test\u001b[39;00m\n\u001b[0;32m      5\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreport_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list(), df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrug_category\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\learn-env\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\learn-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'report_embeddings'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pipelines\n",
        "pipelines = {\n",
        "    \"lr\": Pipeline([('scaler', StandardScaler()), ('clf', LogisticRegression(solver='liblinear', random_state=42))]),\n",
        "    \"rfc\": Pipeline([('clf', RandomForestClassifier(n_estimators=100, random_state=42))]),\n",
        "    \"xgb\": Pipeline([('scaler', StandardScaler()), ('clf', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'))])\n",
        "}"
      ],
      "metadata": {
        "id": "5ceeV92Fm53o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit each model, print classification reports for both training and test data\n",
        "for model_name, pipeline in pipelines.items():\n",
        "    # Fit the model\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on training data and print classification report\n",
        "    y_train_pred = pipeline.predict(X_train)\n",
        "    print(f\"\\nTraining classification report for {model_name}:\")\n",
        "    print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "    # Predict on test data and print classification report\n",
        "    y_test_pred = pipeline.predict(X_test)\n",
        "    print(f\"\\nTest classification report for {model_name}:\")\n",
        "    print(classification_report(y_test, y_test_pred))"
      ],
      "metadata": {
        "id": "ijYysrqxd2IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the GridSearchCV instance\n",
        "grid_search = GridSearchCV(model, param_grid, cv=3)\n",
        "\n",
        "# Tokenize the reports\n",
        "tokenized_reports = df['processed_report'].apply(simple_preprocess)\n",
        "\n",
        "# Train the Word2Vec model and perform grid search\n",
        "grid_search.fit(tokenized_reports)\n",
        "\n",
        "# Get the best model and its hyperparameters\n",
        "best_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "for param, value in best_params.items():\n",
        "    print(f\"{param}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "6XyOHOpaBy5z",
        "outputId": "4264b3de-8dcd-41e1-9da0-2f7ba5e8a2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-54635e4b6bad>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train the Word2Vec model and perform grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_reports\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Get the best model and its hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_multimetric_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \"\"\"\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;34m\"estimator should be an estimator implementing 'fit' method, %r was passed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: estimator should be an estimator implementing 'fit' method, <gensim.models.word2vec.Word2Vec object at 0x7bf6668f2080> was passed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define oversampler\n",
        "oversampler = SMOTE()\n",
        "\n",
        "# Compute class frequencies in 'mixed' column\n",
        "class_freq = df['mixed'].value_counts()\n",
        "\n",
        "# Define class weights based on frequencies\n",
        "class_weights = {value: 10 if value == '0' else 1 for value in class_freq.index}\n",
        "\n",
        "# Encode the labels in y\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Set up the XGBoost parameters\n",
        "params = {\n",
        "    'objective': 'multi:softmax',\n",
        "    'num_class': 3\n",
        "}\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight=class_weights),\n",
        "    'Random Forest': RandomForestClassifier(class_weight=class_weights, n_estimators=100, random_state=42),\n",
        "    'XGB': XGBClassifier(eval_metric='mlogloss')\n",
        "}\n",
        "\n",
        "# Define pipelines\n",
        "pipelines = {\n",
        "    name: make_pipeline(oversampler, model)\n",
        "    for name, model in classifiers.items()\n",
        "}\n",
        "\n",
        "# Split your data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "GRoESyJ3qfoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and evaluate Logistic Regression\n",
        "print('\\nTraining Logistic Regression...')\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline.predict(X_test)\n",
        "print('\\nLogistic Regression:')\n",
        "print(classification_report(y_test, y_pred_lr, target_names=encoder.classes_, zero_division=1))"
      ],
      "metadata": {
        "id": "8uyV3a7yqufm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and evaluate Random Forest\n",
        "print('\\nTraining Random Forest...')\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print('\\nRandom Forest:')\n",
        "print(classification_report(y_test, y_pred_rf, target_names=encoder.classes_, zero_division=1))"
      ],
      "metadata": {
        "id": "VMEvL0TDskUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and evaluate XGBoost with the pipeline\n",
        "print('\\nTraining XGB through pipeline...')\n",
        "pipeline_xgb.fit(X_train, y_train, xgbclassifier__sample_weight=weights_train)\n",
        "y_pred_xgb = pipeline_xgb.predict(X_test)\n",
        "print('\\nXGBoost (through pipeline):')\n",
        "print(classification_report(y_test, y_pred_xgb, target_names=encoder.classes_, zero_division=1))"
      ],
      "metadata": {
        "id": "THJ1mhvLvLT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'penalty': ['l1', 'l2']}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(lr, param_grid, cv=5)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and the best score\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "u_15jvfzOlg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "UuQCDXTehOQt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}